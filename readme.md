# ğŸ§  Image Classification with Custom & Pretrained CNNs

Welcome to this all-in-one pipeline for image classification using PyTorch! Whether you're building from scratch ğŸ› ï¸ or leveraging the power of pretrained ResNet50 âš¡, this project has you covered. Seamlessly configurable, robustly designed, and integrated with Weights & Biases for tracking ğŸ” â€” it's your perfect companion for image classification projects!

---

## ğŸš€ Features

- ğŸ§© **Highly Modular Design**
- ğŸ–¼ï¸ **Supports Custom CNNs & Pretrained ResNet50**
- âš™ï¸ **Flexible Hyperparameter Control via CLI**
- ğŸ§ª **Data Augmentation, Normalization & Caching**
- ğŸ“ˆ **WANDB Integration for Logging & Sweeps**
- ğŸ›ï¸ **Bayesian Optimization for Hyperparameter Tuning**
- ğŸ“Š **Visual Model Summary & Layer-wise Visualization**

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ train.py                  # Main training script
â”œâ”€â”€ cnn.py                   # Custom CNN architecture
â”œâ”€â”€ configuration.py         # Model & optimizer configuration
â”œâ”€â”€ data.py                  # Dataset loading & preprocessing
â”œâ”€â”€ argument_parser.py       # Argument parsing & filter generation
â”œâ”€â”€ sweep_configuration.py   # WandB sweep configs
â”œâ”€â”€ vision_models.py         # Pretrained ResNet50 wrapper
â”œâ”€â”€ shared.py                # Shared globals
â””â”€â”€ README.md                # This file!

```
## ğŸ§ª Getting Started
### ğŸ”§ Installation
Make sure you have the required dependencies installed:

```bash
pip install torch torchvision matplotlib wandb
```
## âš™ï¸ Usage
### ğŸ‹ï¸ Train a Custom CNN
```bash
python train.py
```
## âš™ï¸ Command-Line Arguments Reference

The default values have been configured as per the results obtained from the sweep

| ğŸ·ï¸ **Argument** | ğŸ”¤ **Type** | ğŸ§° **Default** | ğŸ“ **Description** |
|-----------------|------------|----------------|---------------------|
| `--batch_size` `-b` | `int` | `32` | ğŸ“¦ Batch size used for training |
| `--resize` `-r_sz` | `int` | `224` | ğŸ“ Image resize dimension |
| `--filter_automated` `-f_a` | `bool` | `True` | ğŸ§  Enable automated filter configuration |
| `--filter_strategy` `-f_s` | `str` | `'doubled'` | ğŸ” Filter config strategy: `same`, `doubled`, `halved` |
| `--filter_initial` `-f_i` | `int` | `16` | ğŸ§± Number of filters in first layer |
| `--n_convolutions` `-n_c` | `int` | `5` | ğŸ—ï¸ Number of convolution layers |
| `--filter_manual` `-f_m` | `int list` | `[16, 32, 64, 128, 256]` | âš™ï¸ Manual filter config per layer |
| `--padding` `-p` | `bool` | `True` | ğŸ§© Enable padding in conv layers |
| `--stride` `-s` | `int list` | `[1, 1, 1, 1, 1]` | ğŸš¶ Stride for each conv layer |
| `--dense` `-d` | `int list` | `[2048]` | ğŸ§® Neurons in dense layers |
| `--kernel` `-k` | `int list` | `[3, 3, 3, 3, 3]` | ğŸ”¬ Kernel size per conv layer |
| `--conv_activation` `-c_a` | `str` | `'leaky_relu'` | ğŸ”¥ Activation for conv: `relu`, `gelu`, `sigmoid`, etc. |
| `--dense_activation` `-d_a` | `str` | `'relu'` | ğŸŒŸ Activation for dense layers |
| `--n_dense` `-n_d` | `int` | `1` | ğŸ§± Number of dense layers |
| `--optimizer` `-o` | `str` | `'adamax'` | âš™ï¸ Optimizer: `adam`, `sgd`, `nadam`, etc. |
| `--augment` `-a` | `bool` | `True` | ğŸ§ª Enable data augmentation |
| `--batch_norm` `-b_n` | `bool` | `True` | ğŸ§¼ Enable batch normalization |
| `--learning_rate` `-lr` | `float` | `0.0012` | ğŸ“‰ Learning rate |
| `--momentum` `-m` | `float` | `0.849` | ğŸŒ€ Momentum for optimizer |
| `--weight_decay` `-w_d` | `float` | `0.0037` | ğŸ‹ï¸ Weight decay (L2 regularization) |
| `--dropout` `-d_o` | `float` | `0.2` | ğŸ¯ Dropout rate in dense layers |
| `--xavier_init` `-xi` | `bool` | `False` | âœ¨ Use Xavier weight initialization |
| `--wandb` | `flag` | `False` | ğŸ“Š Enable logging to [Weights & Biases](https://wandb.ai) |
| `--wandb_entity` `-we` | `str` | `'da24d008-iit-madras'` | ğŸ§ª W&B entity name |
| `--wandb_project` `-wp` | `str` | `'da6401-assignment2'` | ğŸ“ W&B project name |
| `--wandb_sweep` | `flag` | `False` | ğŸ¯ Enable W&B sweep optimization |
| `--sweep_id` | `str` | `None` | ğŸ†” Sweep ID to resume |
| `--use_pretrained` | `flag` | `False` | ğŸ§  Use pretrained ResNet50 |
| `--pretrained_k` `-pk` | `int` | `1` | ğŸ”„ Number of fine-tuned layers in pretrained model |
| `--use_test` `-ut` | `bool` | `False` | ğŸ§ª Evaluate on test set |


## ğŸ› ï¸ Configuration Highlights
The script is fully CLI-driven using argparse. Here's a glimpse of what you can control:

- ğŸ§± Architecture: --filter_strategy, --n_convolutions, --dense

- ğŸ”¥ Activations: --conv_activation, --dense_activation

- ğŸ¯ Optimizer: --optimizer, --learning_rate, --momentum

- ğŸ›ï¸ Regularization: --dropout, --weight_decay, --batch_norm

- ğŸ§ª Training Options: --augment, --xavier_init, --use_test

- ğŸ–¼ï¸ Preprocessing: --resize, --stride, --padding

For a complete list run 

```bash
python train.py --help
```